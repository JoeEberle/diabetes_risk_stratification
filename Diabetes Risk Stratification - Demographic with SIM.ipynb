{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Diabetes Risk Stratification - Demographic with SIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Pandas is high performance data manipulation \n",
    "import matplotlib.pyplot as plt   # matplot is for python graphics\n",
    "import numpy as np   #numpy is for array processing\n",
    "import seaborn  as sns\n",
    "import story_board as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition ='''\n",
    "\n",
    "## Introduction to Diabetes Analysis - From WEBMD \n",
    "\n",
    "### What Is Diabetes?\n",
    "\n",
    "Diabetes is a number of diseases that involve problems with the hormone insulin. There is no cure for diabetes.\n",
    "\n",
    "### What Are the Different Types of Diabetes?\n",
    "\n",
    "There are three major types of diabetes: **type 1 diabetes**, **type 2 diabetes**, and **gestational diabetes**.\n",
    "\n",
    "### What Is Diabetes Insipidus?\n",
    "\n",
    "Diabetes insipidus causes you to have an almost unquenchable thirst and your body to make a lot of urine.\n",
    "\n",
    "### What Is Gestagenic Diabetes Insipidus?\n",
    "\n",
    "Gestational DI, or gestagenic diabetes insipidus, is a rare disorder that happens in pregnancy, usually in the third trimester.\n",
    "\n",
    "'''\n",
    "\n",
    "sb.start_story(definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition ='''\n",
    "\n",
    "# ðŸ§  Importing the PIMA Indians Dataset - Website KAGGLE.COM\n",
    "\n",
    "The data comes from the National Institute of Diabetes and Digestive and Kidney Diseases but is highly curated upon registry to KAGGLE. \n",
    "\n",
    "The dataset is for the purposes of predicting whether patients have diabetes based on other variables in the dataset. \n",
    "\n",
    "This data is very specific to a sub-population - **IT IS NOT** a representative of a **real world diabetes data set** - it is for learning ONLY. \n",
    "\n",
    "## ðŸ§  The PIMA Indians Data Content \n",
    "\n",
    "The observations in the dataset are records of females over 21 years of age with a Pima Indian heritage and variables \n",
    "such as: blood pressure, insulin levels, body mass index, etc. \n",
    "\n",
    "##  Attributes Normal Value Range\n",
    "1. **Glucose**: Glucose (< 140) = Normal, Glucose (140-200) = Pre-Diabetic, Glucose (> 200) = Diabetic\n",
    "2. **BloodPressure**: B.P (< 60) = Below Normal, B.P (60-80) = Normal, B.P (80-90) = Stage 1 Hypertension, B.P (90-120) = Stage 2 Hypertension, B.P (> 120) = Hypertensive Crisis\n",
    "3. **SkinThickness**: SkinThickness (< 10) = Below Normal, SkinThickness (10-30) = Normal, SkinThickness (> 30) = Above Normal\n",
    "4. **Insulin** : Insulin (< 200) = Normal, Insulin (> 200) = Above Normal\n",
    "5. **Body Mass Index**: BMI (< 18.5) = Underweight, BMI (18.5-25) = Normal, BMI (25-30) = Overweight, BMI (> 30) = Obese\n",
    "\n",
    "'''\n",
    "\n",
    "sb.outmd(definition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"C:/Documents/datasets/diabetes.csv\")\n",
    "dataset = pd.read_csv(\"diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the data set - Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)   #provides top 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape #print the shape of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().values.any()  # determine if any of the dataset is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a correlation Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = dataset.corr()     #establish a correlation matrix for all fields\n",
    "top_correlation_features = correlation_matrix.index\n",
    "plt.figure(figsize=(10,10))\n",
    "g=sns.heatmap(dataset[top_correlation_features].corr(),annot=True,cmap=\"RdYlGn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_map = {True: 1, False: 0}\n",
    "dataset['Diabetes'] = dataset['Outcome'].map(diabetes_map)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_true_count = len(dataset.loc[dataset['Diabetes'] == True])\n",
    "diabetes_false_count = len(dataset.loc[dataset['Diabetes'] == False])\n",
    "(diabetes_true_count,diabetes_false_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "feature_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'SkinThickness']\n",
    "predicted_class = ['Diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[feature_columns].values\n",
    "y = dataset[predicted_class].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total number of rows : {0}\".format(len(dataset)))\n",
    "print(\"number of rows missing Glucose: {0}\".format(len(dataset.loc[dataset['Glucose'] == 0])))\n",
    "print(\"number of rows missing BloodPressure: {0}\".format(len(dataset.loc[dataset['BloodPressure'] == 0])))\n",
    "print(\"number of rows missing Insulin: {0}\".format(len(dataset.loc[dataset['Insulin'] == 0])))\n",
    "print(\"number of rows missing BMI: {0}\".format(len(dataset.loc[dataset['BMI'] == 0])))\n",
    "print(\"number of rows missing DiabetesPedigreeFunction: {0}\".format(len(dataset.loc[dataset['DiabetesPedigreeFunction'] == 0])))\n",
    "print(\"number of rows missing Age: {0}\".format(len(dataset.loc[dataset['Age'] == 0])))\n",
    "print(\"number of rows missing SkinThickness: {0}\".format(len(dataset.loc[dataset['SkinThickness'] == 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "fill_values = SimpleImputer(missing_values=0, strategy=\"mean\")\n",
    "\n",
    "X_train = fill_values.fit_transform(X_train)\n",
    "X_test = fill_values.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Algorithm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest_model = RandomForestClassifier(random_state=10)\n",
    "\n",
    "random_forest_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "            oob_score=False, random_state=10, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train_data = random_forest_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy = {0:.3f}\".format(metrics.accuracy_score(y_test, predict_train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper Parameter Optimization\n",
    "\n",
    "params={\n",
    " \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Explanation of Each Model \n",
       "\n",
       "1. **Logistic Regression**: A linear model used for binary classification that estimates the probability of a sample belonging to a particular class.\n",
       "\n",
       "2. **Decision Tree**: A tree-like model that splits the data into subsets based on the value of input features, making decisions based on feature values to classify instances.\n",
       "\n",
       "3. **K-Nearest Neighbor (KNN)**: A non-parametric method used for classification by finding the 'k' nearest data points in the feature space and assigning the most common class among them to the query point.\n",
       "\n",
       "4. **Gaussian Naive Bayes**: A probabilistic classifier based on Bayes' theorem with the assumption of independence among features, often used for text classification tasks.\n",
       "\n",
       "5. **Multinomial Naive Bayes**: Similar to Gaussian Naive Bayes but specifically designed for classification tasks with discrete features, such as word counts in text classification.\n",
       "\n",
       "6. **Support Vector Classifier (SVC)**: A supervised learning algorithm that finds the hyperplane that best separates classes in a high-dimensional space, often used for binary classification.\n",
       "\n",
       "7. **Random Forest**: An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees.\n",
       "\n",
       "8. **XGBoost**: An optimized gradient boosting library that implements machine learning algorithms under the Gradient Boosting framework, known for its speed and performance in handling large datasets.\n",
       "\n",
       "9. **Multi-layer Perceptron (MLP)**: A type of artificial neural network composed of multiple layers of nodes (neurons) that can learn non-linear relationships between input and output data.\n",
       "\n",
       "10. **Gradient Boosting Classifier**: A machine learning technique that builds an ensemble of weak learners (typically decision trees) in a sequential manner, with each tree correcting the errors of its predecessors, resulting in a strong predictive model.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import story_board as sb\n",
    "definition ='''\n",
    "## Explanation of Each Model \n",
    "\n",
    "1. **Logistic Regression**: A linear model used for binary classification that estimates the probability of a sample belonging to a particular class.\n",
    "\n",
    "2. **Decision Tree**: A tree-like model that splits the data into subsets based on the value of input features, making decisions based on feature values to classify instances.\n",
    "\n",
    "3. **K-Nearest Neighbor (KNN)**: A non-parametric method used for classification by finding the 'k' nearest data points in the feature space and assigning the most common class among them to the query point.\n",
    "\n",
    "4. **Gaussian Naive Bayes**: A probabilistic classifier based on Bayes' theorem with the assumption of independence among features, often used for text classification tasks.\n",
    "\n",
    "5. **Multinomial Naive Bayes**: Similar to Gaussian Naive Bayes but specifically designed for classification tasks with discrete features, such as word counts in text classification.\n",
    "\n",
    "6. **Support Vector Classifier (SVC)**: A supervised learning algorithm that finds the hyperplane that best separates classes in a high-dimensional space, often used for binary classification.\n",
    "\n",
    "7. **Random Forest**: An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or the mean prediction (regression) of the individual trees.\n",
    "\n",
    "8. **XGBoost**: An optimized gradient boosting library that implements machine learning algorithms under the Gradient Boosting framework, known for its speed and performance in handling large datasets.\n",
    "\n",
    "9. **Multi-layer Perceptron (MLP)**: A type of artificial neural network composed of multiple layers of nodes (neurons) that can learn non-linear relationships between input and output data.\n",
    "\n",
    "10. **Gradient Boosting Classifier**: A machine learning technique that builds an ensemble of weak learners (typically decision trees) in a sequential manner, with each tree correcting the errors of its predecessors, resulting in a strong predictive model.\n",
    "\n",
    "'''\n",
    "\n",
    "sb.outmd(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter optimization using RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=xgboost.XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Here we go\n",
    "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
    "random_search.fit(X,y.ravel())\n",
    "timer(start_time) # timing ends here for \"start_time\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.3, gamma=0.0, learning_rate=0.25,\n",
    "       max_delta_step=0, max_depth=3, min_child_weight=7, missing=None,\n",
    "       n_estimators=100, n_jobs=1, nthread=None,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
    "       subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score=cross_val_score(classifier,X,y.ravel(),cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
